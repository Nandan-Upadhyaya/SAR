# Terrain-Aware Synthetic Aperture Radar Image (SAR) to RGB Colorization through Automated Terrain Classification using Conditional Generative Adversarial Network (GAN)

## üõ∞Ô∏è Overview

This project implements a deep learning system for translating Synthetic Aperture Radar (SAR) imagery into realistic RGB images. A pretrained ResNet34 Model is finetuned to classify different terrains for SAR Images. Integrating the ResNet34 Model with a conditional Generative Adversarial Network (GAN) architecture, the model can generate colorized versions of SAR data that closely resemble aerial/satellite photography.



## üîç Features

- **Terrain-Conditional Image Translation**: Generates RGB images conditioned on terrain classification
- **Multi-Terrain Support**: Urban, Grassland, Agricultural, and Barren Land domains
- **UNet-Based Generator**: Advanced encoder-decoder architecture with skip connections
- **PatchGAN Discriminator**: For high-quality local texture assessment
- **Performance Metrics**: FID, SSIM, PSNR, and Inception Score (IS) evaluation

## üß† Architecture

<p align="center">
  <img src="Arch.png" alt="Architecture Diagram" width="800"/>
</p>


The above figure depicts the architectural diagram that is used for the purpose.
The model is built in two stages:

1. Terrain Classification
A ResNet34-based classifier takes in SAR images and predicts the terrain type (e.g., urban, grassland, agricultural, barrenland).

The predicted terrain is converted into a one-hot encoded vector and passed through a small neural network to generate a terrain embedding.

2. Terrain-Aware Colorization (UNet + PatchGAN)
A UNet-based generator takes the SAR image and terrain embedding to generate a colorized version of the input.

A PatchGAN discriminator evaluates the realism of the generated image, conditioned on both the SAR input and terrain type.

## üèóÔ∏è Training Setup:
- **Framework** : PyTorch
- **GPU** : NVIDIA Geforce RTX 4060
- **Epochs** : 140
- **Batch Size** : 8
- **Learning Rate** : 2e-4
- **Mixed Precision** : Enabled

## üèóÔ∏è Dataset:
The dataset consisted of 4 terrains namely, urban, grassland, barrenland and agri. Each terrain had 2 different folders, one for the SAR Images and the other included the Corresponding color images of it.
The dataset was split in a 80/20 manner during training.

## üöÄ Getting Started

### Requirements

```
torch>=1.8.0
torchvision>=0.9.0
numpy
matplotlib
Pillow
tqdm
scikit-image
```

  


## üìä Results

Our model achieves state-of-the-art results in SAR-to-RGB conversion across multiple terrain types. By conditioning on terrain classification, the model produces more accurate and visually coherent colorized images than terrain-agnostic approaches.
We evaluated our model using multiple metrics:
- **FID** (Fr√©chet Inception Distance): Measures similarity between generated and real images
- **SSIM** (Structural Similarity Index): Evaluates structural preservation
- **PSNR** (Peak Signal-to-Noise Ratio): Assesses pixel-level reconstruction quality
- **IS** (Inception Score): Measures diversity and quality of generated samples

Classification accuracy achieved by the ResNet34 Model : **99.94%**
### Key Metrics Achieved by the GAN Model:
- **FID Score**: 108.18 
- **SSIM**: 0.36
- **PSNR**: 19 dB
- **IS** : 3.07


<p align="center">
  <img src="random_terrain_comparison.jpg" alt="SAR to RGB Translation Examples" width="800"/>
</p>
 The above SAR Images are taken from 4 different terrains, shuffled and sent to the model for prediction.
The second column represents the ground truth images of all the input SAR Images and the third column represents the images which are generated by the model.
<br>
<p align="center">
  <img src="ROIs1868_summer_s1_59_p8_colorized_comparison.jpg" alt="SAR to RGB Translation Examples" width="700"/>
</p>
SAR Input Image provided by the user and the corresponding output colorized image provided by our model.
















